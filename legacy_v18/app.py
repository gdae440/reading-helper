import streamlit as st
from openai import OpenAI
import edge_tts
from gtts import gTTS
import asyncio
import json
import os
import io
import base64
import re
from datetime import datetime
from PIL import Image
import genanki
from streamlit_option_menu import option_menu
import random

# ================= 1. æ ¸å¿ƒé…ç½® =================

def load_config():
    return {
        "api_key": "",
        "engine": "Edge (æ¨è)",
        "voice_role": "en-GB-RyanNeural",
        "speed": 0,
        "learn_lang": "ğŸ‡¬ğŸ‡§ è‹±è¯­",
        "chat_model": "deepseek-ai/DeepSeek-V3",
        "ocr_model": "Qwen/Qwen2.5-VL-72B-Instruct",
        "generic_api_key": "",
        "generic_base_url": "https://api.siliconflow.cn/v1"
    }

def load_vocab():
    VOCAB_FILE = "../my_vocab.json" 
    if not os.path.exists(VOCAB_FILE):
        VOCAB_FILE = "my_vocab.json"
    if os.path.exists(VOCAB_FILE):
        try: return json.load(open(VOCAB_FILE, "r", encoding="utf-8"))
        except: return []
    return []

def save_vocab(vocab_list):
    VOCAB_FILE = "../my_vocab.json"
    try:
        with open(VOCAB_FILE, "w", encoding="utf-8") as f:
            json.dump(vocab_list, f, ensure_ascii=False, indent=2)
    except:
        try:
            with open("my_vocab.json", "w", encoding="utf-8") as f:
                json.dump(vocab_list, f, ensure_ascii=False, indent=2)
        except: pass

def get_smart_filename(text):
    if not text: return f"read_aloud_{datetime.now().strftime('%H%M%S')}.mp3"
    snippet = text[:50]
    safe_name = re.sub(r'[^\w\s\u4e00-\u9fa5-]', '', snippet)
    safe_name = re.sub(r'[\s]+', '_', safe_name).strip()
    if not safe_name: return f"read_aloud_{datetime.now().strftime('%H%M%S')}.mp3"
    return f"{safe_name}.mp3"

# ================= 2. çŠ¶æ€ç®¡ç† =================

st.set_page_config(page_title="è·Ÿè¯»åŠ©æ‰‹ Pro", layout="wide", page_icon="ğŸ“˜")

if 'cfg' not in st.session_state:
    init_cfg = load_config()
    env_key = os.getenv("SILICON_KEY")
    if env_key: init_cfg["api_key"] = env_key
    st.session_state.cfg = init_cfg

if 'vocab' not in st.session_state: st.session_state.vocab = load_vocab()
if 'main_text' not in st.session_state: st.session_state.main_text = ""
if 'trans_text' not in st.session_state: st.session_state.trans_text = ""
if 'audio_data' not in st.session_state: st.session_state.audio_data = None
if 'last_lookup' not in st.session_state: st.session_state.last_lookup = None
# æ–°å¢ï¼šä¸“é—¨ç”¨äºæŸ¥è¯å‘éŸ³çš„çŠ¶æ€ï¼Œé˜²æ­¢åˆ·æ–°ä¸¢å¤±
if 'lookup_audio' not in st.session_state: st.session_state.lookup_audio = None 

# ================= 3. æ ¸å¿ƒé€»è¾‘ =================

def get_api_client(cfg):
    key = cfg.get("generic_api_key") if cfg.get("generic_api_key") else cfg.get("api_key")
    base_url = cfg.get("generic_base_url") if cfg.get("generic_base_url") else "https://api.siliconflow.cn/v1"
    if not key: return None, "æœªé…ç½® API Key"
    return OpenAI(api_key=key, base_url=base_url), None

def api_call(type, content, cfg):
    client, err = get_api_client(cfg)
    if not client: return None, err

    chat_model = cfg.get("chat_model", "deepseek-ai/DeepSeek-V3")
    ocr_model = cfg.get("ocr_model", "Qwen/Qwen2.5-VL-72B-Instruct")

    try:
        if type == "ocr":
            buffered = io.BytesIO()
            content.save(buffered, format="JPEG", quality=85)
            b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            res = client.chat.completions.create(model=ocr_model, messages=[{"role": "user", "content": [{"type": "text", "text": "OCR text only. Keep formatting."}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}]}])
            return res.choices[0].message.content, None
        elif type == "lookup":
            prompt = """Dictionary API. User input: "{content}". Return JSON: {{ "detected_lang": "...", "ipa": "...", "zh": "...", "ru": "..." }} (Concise)"""
            res = client.chat.completions.create(model=chat_model, messages=[{"role": "user", "content": prompt}], response_format={"type": "json_object"})
            return json.loads(res.choices[0].message.content), None
        elif type == "trans":
            res = client.chat.completions.create(model=chat_model, messages=[{"role": "user", "content": f"Translate to Chinese (Natural & Concise):\\n\\n{content}"}])
            return res.choices[0].message.content, None
    except Exception as e:
        return None, f"API Error: {str(e)}"
    return None, "Unknown Error"

async def get_audio_bytes_mixed(text, engine_type, voice_id, speed_int, cfg):
    if "Edge" in engine_type:
        try:
            communicate = edge_tts.Communicate(text, voice_id, rate=f"{speed_int:+d}%")
            mp3_fp = io.BytesIO()
            async for chunk in communicate.stream():
                if chunk["type"] == "audio": mp3_fp.write(chunk["data"])
            return mp3_fp.getvalue(), None
        except Exception as e: return None, f"Edge Error: {e}"
    elif "SiliconFlow" in engine_type:
        client, err = get_api_client(cfg)
        if not client: return None, err
        model_id = voice_id.split(":")[0] if ":" in voice_id else "FunAudioLLM/CosyVoice2-0.5B"
        try:
            sf_speed = 1.0 + (speed_int / 100.0)
            response = client.audio.speech.create(model=model_id, voice=voice_id, input=text, speed=sf_speed)
            return response.content, None
        except Exception as e: return None, f"SF Error: {e}"
    elif "Google" in engine_type:
        try:
            lang_map = {"ğŸ‡¬ğŸ‡§ è‹±è¯­": "en", "ğŸ‡«ğŸ‡· æ³•è¯­": "fr", "ğŸ‡©ğŸ‡ª å¾·è¯­": "de", "ğŸ‡·ğŸ‡º ä¿„è¯­": "ru"}
            lang_code = lang_map.get(cfg["learn_lang"], "en")
            tts = gTTS(text=text, lang=lang_code)
            mp3_fp = io.BytesIO(); tts.write_to_fp(mp3_fp)
            return mp3_fp.getvalue(), None
        except Exception as e: return None, f"Google Error: {e}"
    return None, "Unknown Engine"

# ================= 4. UI æ¸²æŸ“ =================

# CSS å‡çº§ï¼šæ›´å¹²å‡€çš„è¾¹æ¡†ï¼Œç§»é™¤å¤æ‚çš„è£…é¥°ï¼Œå¢åŠ é—´è·
st.markdown("""
<style>
    .stButton > button { width: 100%; border-radius: 6px; height: 2.5rem; }
    div[data-testid="stVerticalBlock"] > div > button[kind="primary"] {
        background-color: #007AFF; color: white; border: none;
    }
    div.lookup-card {
        border: 1px solid #e5e5e5; border-radius: 8px; padding: 20px;
        background-color: white; margin-bottom: 15px;
    }
    @media (prefers-color-scheme: dark) {
        div.lookup-card { background-color: #1e1e1e; border-color: #333; }
    }
    h1, h2, h3 { font-weight: 600; }
    .small-font { font-size: 12px; color: #666; }
</style>
""", unsafe_allow_html=True)

# --- ä¾§è¾¹æ ï¼šå¯¼èˆª + è¯­éŸ³æ§åˆ¶ ---
with st.sidebar:
    st.markdown("### è·Ÿè¯»åŠ©æ‰‹ Pro")
    selected = option_menu(None, ["å­¦ä¹ ä¸»é¡µ", "å•è¯æœ¬", "è®¾ç½®"], 
        icons=['book', 'bookmark', 'sliders'], menu_icon="cast", default_index=0,
        styles={"nav-link": {"font-size": "14px", "text-align": "left", "margin":"0px"}})
    
    st.divider()
    
    # è¯­éŸ³é…ç½®ç›´æ¥æ”¾ä¾§è¾¹æ ï¼Œæ–¹ä¾¿éšæ—¶è°ƒ
    st.markdown("#### ğŸ”Š è¯­éŸ³æ§åˆ¶")
    
    lang_opts = ["ğŸ‡¬ğŸ‡§ è‹±è¯­", "ğŸ‡«ğŸ‡· æ³•è¯­", "ğŸ‡©ğŸ‡ª å¾·è¯­", "ğŸ‡·ğŸ‡º ä¿„è¯­"]
    # ä¿æŒä¹‹å‰é€‰ä¸­çš„è¯­è¨€
    curr_lang_idx = 0
    if st.session_state.cfg["learn_lang"] in lang_opts:
        curr_lang_idx = lang_opts.index(st.session_state.cfg["learn_lang"])
    
    new_lang = st.selectbox("ç›®æ ‡è¯­è¨€", lang_opts, index=curr_lang_idx)
    if new_lang != st.session_state.cfg["learn_lang"]:
        st.session_state.cfg["learn_lang"] = new_lang
        # è¯­è¨€å˜äº†ï¼Œé‡ç½®ä¸€ä¸‹å‘éŸ³äºº
        if "è‹±è¯­" in new_lang: st.session_state.cfg["voice_role"] = "en-GB-RyanNeural"
        elif "æ³•è¯­" in new_lang: st.session_state.cfg["voice_role"] = "fr-FR-HenriNeural"
        elif "å¾·è¯­" in new_lang: st.session_state.cfg["voice_role"] = "de-DE-ConradNeural"
        elif "ä¿„è¯­" in new_lang: st.session_state.cfg["voice_role"] = "ru-RU-DmitryNeural"
        st.rerun()

    eng_opts = ["Edge (æ¨è)", "SiliconFlow", "Google"]
    curr_eng_idx = 0
    if st.session_state.cfg["engine"] in eng_opts:
        curr_eng_idx = eng_opts.index(st.session_state.cfg["engine"])
    st.session_state.cfg["engine"] = st.selectbox("è¯­éŸ³å¼•æ“", eng_opts, index=curr_eng_idx)
    
    # æ ¹æ®å¼•æ“æ˜¾ç¤ºä¸åŒçš„éŸ³è‰²é€‰é¡¹
    if "Edge" in st.session_state.cfg["engine"]:
        # ç®€å•çš„éŸ³è‰²æ˜ å°„
        voice_map = {
            "ğŸ‡¬ğŸ‡§ è‹±è¯­": {"Ryan (è‹±/ç”·)": "en-GB-RyanNeural", "Aria (ç¾/å¥³)": "en-US-AriaNeural"},
            "ğŸ‡«ğŸ‡· æ³•è¯­": {"Henri (æ³•/ç”·)": "fr-FR-HenriNeural", "Denise (æ³•/å¥³)": "fr-FR-DeniseNeural"},
            "ğŸ‡©ğŸ‡ª å¾·è¯­": {"Conrad (å¾·/ç”·)": "de-DE-ConradNeural", "Katja (å¾·/å¥³)": "de-DE-KatjaNeural"},
            "ğŸ‡·ğŸ‡º ä¿„è¯­": {"Dmitry (ä¿„/ç”·)": "ru-RU-DmitryNeural", "Svetlana (ä¿„/å¥³)": "ru-RU-SvetlanaNeural"},
        }
        current_voices = voice_map.get(st.session_state.cfg["learn_lang"], {})
        voice_names = list(current_voices.keys())
        if voice_names:
            # å°è¯•æ‰¾åˆ°å½“å‰éŸ³è‰²å¯¹åº”çš„åå­—
            curr_v_name = voice_names[0]
            for name, code in current_voices.items():
                if code == st.session_state.cfg["voice_role"]:
                    curr_v_name = name
                    break
            selected_v_name = st.selectbox("é€‰æ‹©éŸ³è‰²", voice_names, index=voice_names.index(curr_v_name) if curr_v_name in voice_names else 0)
            st.session_state.cfg["voice_role"] = current_voices[selected_v_name]
            
    elif "SiliconFlow" in st.session_state.cfg["engine"]:
        sf_voices = {
            "Benjamin (è‹±/ç”·)": "FunAudioLLM/CosyVoice2-0.5B:benjamin",
            "Alex (ç¾/ç”·)": "FunAudioLLM/CosyVoice2-0.5B:alex",
            "Bella (ç¾/å¥³)": "FunAudioLLM/CosyVoice2-0.5B:bella"
        }
        sf_names = list(sf_voices.keys())
        curr_sf = sf_names[0]
        for name, code in sf_voices.items():
            if code == st.session_state.cfg["voice_role"]:
                curr_sf = name
        sel_sf = st.selectbox("é€‰æ‹©éŸ³è‰²", sf_names, index=sf_names.index(curr_sf) if curr_sf in sf_names else 0)
        st.session_state.cfg["voice_role"] = sf_voices[sel_sf]

    st.session_state.cfg["speed"] = st.slider("è¯­é€Ÿè°ƒèŠ‚", -50, 50, st.session_state.cfg["speed"], step=10)


# --- ä¸»ç•Œé¢é€»è¾‘ ---

if selected == "è®¾ç½®":
    st.subheader("å…¨å±€è®¾ç½®")
    
    # ä½¿ç”¨ tabs æ•´ç†å¸ƒå±€
    tab1, tab2 = st.tabs(["ğŸ”‘ API é…ç½®", "ğŸ¤– æ¨¡å‹å‚æ•°"])
    
    with tab1:
        st.info("æ¨èä¼˜å…ˆä½¿ç”¨å¤‡ç”¨ APIï¼Œæ”¯æŒè‡ªå®šä¹‰ OpenAI æ ¼å¼æ¥å£ã€‚")
        c1, c2 = st.columns(2)
        with c1:
            st.session_state.cfg["generic_base_url"] = st.text_input("API Base URL", value=st.session_state.cfg.get("generic_base_url", "https://api.siliconflow.cn/v1"))
        with c2:
            st.session_state.cfg["generic_api_key"] = st.text_input("API Key", value=st.session_state.cfg.get("generic_api_key", ""), type="password")
            
        with st.expander("æ—§ç‰ˆ SiliconFlow åŸç”Ÿé…ç½® (å¯é€‰)"):
            st.session_state.cfg["api_key"] = st.text_input("SiliconFlow Key", value=st.session_state.cfg["api_key"], type="password")

    with tab2:
        c3, c4 = st.columns(2)
        with c3: st.session_state.cfg["chat_model"] = st.text_input("Chat æ¨¡å‹åç§°", value=st.session_state.cfg["chat_model"])
        with c4: st.session_state.cfg["ocr_model"] = st.text_input("OCR æ¨¡å‹åç§°", value=st.session_state.cfg["ocr_model"])

elif selected == "å­¦ä¹ ä¸»é¡µ":
    col_l, col_r = st.columns([2, 1])
    
    with col_l:
        st.caption("é˜…è¯»ä¸æœ—è¯»")
        mode = st.radio("è¾“å…¥æ¨¡å¼", ["æ–‡æœ¬", "OCR æ‹ç…§"], horizontal=True, label_visibility="collapsed")
        
        if mode == "æ–‡æœ¬":
            txt_input = st.text_area("å†…å®¹è¾“å…¥", height=200, label_visibility="collapsed", placeholder="åœ¨æ­¤è¾“å…¥æˆ–ç²˜è´´æ–‡æœ¬...")
            if st.button("ç¡®è®¤å†…å®¹", use_container_width=True):
                st.session_state.main_text = txt_input
                st.session_state.trans_text = ""
                st.rerun()
        else:
            up = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=['jpg','png'], label_visibility="collapsed")
            if up and st.button("å¼€å§‹è¯†åˆ«", use_container_width=True):
                with st.spinner("AI è¯†åˆ«ä¸­..."):
                    res, err = api_call("ocr", Image.open(up), st.session_state.cfg)
                    if res: 
                        st.session_state.main_text = res
                        st.session_state.trans_text = ""
                        st.rerun()
                    else: st.error(err)

        if st.session_state.main_text:
            st.markdown("---")
            st.markdown(f"**åŸæ–‡å†…å®¹ï¼š**\n\n{st.session_state.main_text}")
            
            if st.session_state.trans_text:
                st.info(f"**è¯‘æ–‡ï¼š**\n\n{st.session_state.trans_text}")

            # æ“ä½œæ 
            c_act1, c_act2 = st.columns(2)
            with c_act1:
                if st.button("â–¶ï¸ æœ—è¯»å…¨æ–‡", type="primary", use_container_width=True):
                    with st.spinner("ç”Ÿæˆè¯­éŸ³..."):
                        ab, err = asyncio.run(get_audio_bytes_mixed(
                            st.session_state.main_text, 
                            st.session_state.cfg["engine"], 
                            st.session_state.cfg["voice_role"], 
                            st.session_state.cfg["speed"], 
                            st.session_state.cfg
                        ))
                        if ab: 
                            st.session_state.audio_data = ab
                            st.rerun()
                        else: st.error(err)
            with c_act2:
                if st.button("ğŸ“ å…¨æ–‡ç¿»è¯‘", use_container_width=True):
                    with st.spinner("ç¿»è¯‘ä¸­..."):
                        trans, err = api_call("trans", st.session_state.main_text, st.session_state.cfg)
                        if trans:
                            st.session_state.trans_text = trans
                            st.rerun()
                        else: st.error(err)

            if st.session_state.audio_data:
                st.audio(st.session_state.audio_data, format='audio/mpeg')

    with col_r:
        st.caption("æ™ºèƒ½æŸ¥è¯")
        # ä½¿ç”¨ form é¿å…æ¯æ¬¡è¾“å…¥å­—ç¬¦éƒ½åˆ·æ–°
        with st.form("lookup_form"):
            q_w = st.text_input("å•è¯", placeholder="è¾“å…¥å•è¯...")
            submitted = st.form_submit_button("æŸ¥è¯¢", use_container_width=True)
            
        if submitted and q_w:
            with st.spinner("æŸ¥è¯¢ä¸­..."):
                info, err = api_call("lookup", q_w, st.session_state.cfg)
                if info:
                    info["word"] = q_w
                    st.session_state.last_lookup = info
                    st.session_state.lookup_audio = None # é‡ç½®å‘éŸ³
                    
                    # è‡ªåŠ¨ä¿å­˜
                    exists = any(i['word'] == q_w for i in st.session_state.vocab)
                    if not exists:
                        st.session_state.vocab.insert(0, {"word": q_w, "lang": st.session_state.cfg["learn_lang"], "date": datetime.now().strftime("%Y-%m-%d"), **info})
                        save_vocab(st.session_state.vocab)
                    st.rerun()
                else:
                    st.error(err)

        if st.session_state.last_lookup:
            ll = st.session_state.last_lookup
            # ç®€æ´çš„å¡ç‰‡å±•ç¤º
            st.markdown(f"""
            <div class="lookup-card">
                <h3 style="margin:0">{ll['word']}</h3>
                <div style="color:#666; margin-bottom:10px; font-family:monospace;">[{ll.get('ipa','--')}]</div>
                <div style="margin-bottom:5px"><b>ğŸ‡¨ğŸ‡³</b> {ll.get('zh','--')}</div>
                <div><b>ğŸ‡·ğŸ‡º</b> {ll.get('ru','--')}</div>
            </div>
            """, unsafe_allow_html=True)
            
            # å‘éŸ³æŒ‰é’®
            if st.button("ğŸ”Š æœ—è¯»å•è¯", use_container_width=True):
                ab, _ = asyncio.run(get_audio_bytes_mixed(ll['word'], "Edge (æ¨è)", "en-US-AriaNeural", 0, st.session_state.cfg))
                if ab:
                    st.session_state.lookup_audio = ab
                    st.rerun() # å…³é”®ï¼šåˆ·æ–°ä»¥æ˜¾ç¤ºéŸ³é¢‘æ’­æ”¾å™¨
            
            # ç¨³å®šçš„éŸ³é¢‘æ’­æ”¾å™¨
            if st.session_state.lookup_audio:
                st.audio(st.session_state.lookup_audio, format="audio/mpeg", autoplay=True)

elif selected == "å•è¯æœ¬":
    st.subheader(f"æˆ‘çš„ç”Ÿè¯æœ¬ ({len(st.session_state.vocab)})")
    if not st.session_state.vocab:
        st.info("ç©ºç©ºå¦‚ä¹Ÿã€‚åœ¨å³ä¾§æŸ¥è¯è‡ªåŠ¨æ·»åŠ ã€‚")
    else:
        for i, item in enumerate(st.session_state.vocab):
            with st.expander(f"{item['word']}", expanded=False):
                st.write(f"[{item.get('ipa','')}]")
                st.write(f"ğŸ‡¨ğŸ‡³ {item.get('zh','')}")
                st.write(f"ğŸ‡·ğŸ‡º {item.get('ru','')}")
                c_del, c_play = st.columns([1, 4])
                with c_del:
                    if st.button("ğŸ—‘ï¸ åˆ é™¤", key=f"d_{i}"):
                        st.session_state.vocab.pop(i)
                        save_vocab(st.session_state.vocab)
                        st.rerun()