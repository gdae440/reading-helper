import streamlit as st
from openai import OpenAI
import edge_tts
from gtts import gTTS
import asyncio
import json
import os
import io
import base64
import re
import random
import time
from datetime import datetime
from PIL import Image
import genanki
from streamlit_option_menu import option_menu

# ================= 1. æ ¸å¿ƒé…ç½®ä¸å·¥å…·å‡½æ•° =================

VOCAB_FILE = "my_vocab.json"

def load_vocab():
    """åŠ è½½ç”Ÿè¯æœ¬"""
    paths = ["my_vocab.json", "../my_vocab.json"]
    for p in paths:
        if os.path.exists(p):
            try:
                return json.load(open(p, "r", encoding="utf-8"))
            except: pass
    return []

def save_vocab(vocab_list):
    """ä¿å­˜ç”Ÿè¯æœ¬"""
    try:
        with open(VOCAB_FILE, "w", encoding="utf-8") as f:
            json.dump(vocab_list, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ä¿å­˜å¤±è´¥: {e}")

def get_smart_filename(text):
    """ç”Ÿæˆæ–‡ä»¶å"""
    if not text: return "audio.mp3"
    snippet = text[:20]
    safe_name = re.sub(r'[^\w\s\u4e00-\u9fa5-]', '', snippet).strip()
    return f"{safe_name}.mp3" if safe_name else "audio.mp3"

def auto_detect_language(text):
    """ç®€å•çš„è¯­è¨€è‡ªåŠ¨æ£€æµ‹ (æ— éœ€é¢å¤–ä¾èµ–)"""
    if not text: return None
    # æ£€æµ‹è¥¿é‡Œå°”å­—æ¯ -> ä¿„è¯­
    if re.search(r'[\u0400-\u04FF]', text):
        return "ğŸ‡·ğŸ‡º ä¿„è¯­"
    # æ£€æµ‹æ±‰å­— -> ä¸­æ–‡ (é€šå¸¸ä½œä¸ºæ¯è¯­æˆ–å­¦ä¹ å¯¹è±¡)
    elif re.search(r'[\u4e00-\u9fa5]', text):
        # è¿™é‡Œå‡è®¾å¦‚æœæ˜¯ä¸­æ–‡æ–‡ç« ï¼Œå¯èƒ½ä¸éœ€è¦æœ—è¯»ï¼Œæˆ–è€…æ˜¯æƒ³ç”¨ä¸­æ–‡è¯­éŸ³
        # æš‚æ—¶ä¸è‡ªåŠ¨åˆ‡åˆ°ä¸­æ–‡ï¼Œé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ã€‚
        # æˆ‘ä»¬çš„ç›®æ ‡è¯­è¨€åˆ—è¡¨ä¸­æ²¡æœ‰ä¸­æ–‡ï¼Œæ‰€ä»¥æš‚æ—¶å¿½ç•¥ï¼Œæˆ–é»˜è®¤ä¸ºè‹±è¯­
        pass 
    # æ£€æµ‹æ³•è¯­/å¾·è¯­ç‰¹æ®Šå­—ç¬¦ (ç®€å•åˆ¤æ–­)
    elif re.search(r'[Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã¶Ã¹Ã»Ã¼Ã§ÃŸ]', text, re.IGNORECASE):
        # è¿™ç§åˆ¤æ–­æ¯”è¾ƒç²—ç³™ï¼Œä½†åœ¨ä¸­/è‹±/ä¿„ç¯å¢ƒä¸‹è¶³å¤ŸåŒºåˆ†ä¿„è¯­
        # å¦‚æœæ˜¯å¾·è¯­/æ³•è¯­æ··åˆå¾ˆéš¾åŒºåˆ†ï¼Œè¿™é‡Œä¼˜å…ˆä¸åšè¯¯åˆ¤
        pass
    return "ğŸ‡¬ğŸ‡§ è‹±è¯­" # é»˜è®¤

# ================= 2. é¡µé¢åˆå§‹åŒ–ä¸æ ·å¼ =================

st.set_page_config(page_title="è·Ÿè¯»åŠ©æ‰‹ Pro", layout="wide", page_icon="ğŸ“˜")

st.markdown("""
<style>
    :root {
        --bg-color: #f5f5f7;
        --text-color: #000000;
        --card-bg-color: #ffffff;
        --card-border-color: #e5e5ea;
        --primary-color: #007aff;
        --secondary-text-color: #666;
        --shadow-color: rgba(0,0,0,0.1);
        --shadow-color-light: rgba(0,0,0,0.05);
    }
    @media (prefers-color-scheme: dark) {
        :root {
            --bg-color: #1c1c1e;
            --text-color: #ffffff;
            --card-bg-color: #2c2c2e;
            --card-border-color: #3a3a3c;
            --primary-color: #0a84ff;
            --secondary-text-color: #8e8e93;
            --shadow-color: rgba(255,255,255,0.1);
            --shadow-color-light: rgba(255,255,255,0.05);
        }
    }
    .stApp {
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: -apple-system, BlinkMacSystemFont, sans-serif;
    }
    .stButton > button {
        border-radius: 10px;
        border: none;
        font-weight: 500;
        transition: 0.2s;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px var(--shadow-color);
    }
    div.lookup-card {
        background: var(--card-bg-color);
        border-radius: 12px;
        padding: 20px;
        box-shadow: 0 2px 10px var(--shadow-color-light);
        margin-bottom: 15px;
        border: 1px solid var(--card-border-color);
        color: var(--text-color);
    }
    .lookup-card h3 {
        color: var(--primary-color) !important;
    }
    .lookup-card div {
        color: var(--text-color) !important;
    }
    .lookup-card div[style*="color:#666"] {
        color: var(--secondary-text-color) !important;
    }
</style>
""", unsafe_allow_html=True)

# ================= 3. çŠ¶æ€ç®¡ç† =================

if 'cfg' not in st.session_state:
    st.session_state.cfg = {
        "api_key": os.getenv("SILICON_KEY", ""),
        "engine": "Edge (æ¨è)",
        "voice_role": "en-US-AriaNeural",
        "speed": 0,
        "learn_lang": "ğŸ‡¬ğŸ‡§ è‹±è¯­",
        "chat_model": "deepseek-ai/DeepSeek-V3",
        "ocr_model": "Qwen/Qwen2.5-VL-72B-Instruct",
        "generic_base_url": "https://api.siliconflow.cn/v1"
    }

if 'vocab' not in st.session_state: st.session_state.vocab = load_vocab()
if 'main_text' not in st.session_state: st.session_state.main_text = ""
if 'trans_text' not in st.session_state: st.session_state.trans_text = ""
if 'audio_data' not in st.session_state: st.session_state.audio_data = None
# å¢åŠ æ—¶é—´æˆ³ key å¼ºåˆ¶åˆ·æ–°æ’­æ”¾å™¨
if 'audio_timestamp' not in st.session_state: st.session_state.audio_timestamp = 0 
if 'last_lookup' not in st.session_state: st.session_state.last_lookup = None
if 'lookup_audio' not in st.session_state: st.session_state.lookup_audio = None
if 'lookup_audio_ts' not in st.session_state: st.session_state.lookup_audio_ts = 0

# ================= 4. æ ¸å¿ƒé€»è¾‘ =================

def get_api_client(cfg):
    key = cfg.get("api_key")
    base_url = cfg.get("generic_base_url", "https://api.siliconflow.cn/v1")
    if not key: return None, "âŒ æœªé…ç½® API Key"
    if not base_url.endswith("/v1"): 
        base_url = base_url.rstrip("/") + "/v1"
    return OpenAI(api_key=key, base_url=base_url), None

def api_call(task_type, content, cfg):
    client, err = get_api_client(cfg)
    if not client: return None, err

    try:
        if task_type == "ocr":
            buffered = io.BytesIO()
            content.save(buffered, format="JPEG", quality=85)
            b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            res = client.chat.completions.create(
                model=cfg["ocr_model"],
                messages=[{"role": "user", "content": [
                    {"type": "text", "text": "OCR raw text output only. No markdown."}, 
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}
                ]}]
            )
            return res.choices[0].message.content, None
        elif task_type == "lookup":
            prompt = f"""Explain '{content}' concisely. Format strictly as valid JSON:
            {{ "word": "{content}", "ipa": "/.../", "zh": "ä¸­æ–‡é‡Šä¹‰", "ru": "ä¿„è¯­é‡Šä¹‰ (if applicable)" }}"""
            res = client.chat.completions.create(
                model=cfg["chat_model"],
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content), None
        elif task_type == "trans":
            res = client.chat.completions.create(
                model=cfg["chat_model"],
                messages=[{"role": "user", "content": f"Translate to natural Chinese:\n{content}"}]
            )
            return res.choices[0].message.content, None
    except Exception as e:
        return None, str(e)
    return None, "Unknown"

async def get_audio_bytes_mixed(text, engine_type, voice_id, speed_int, cfg):
    """TTS æ ¸å¿ƒç”Ÿæˆ, å¸¦å›é€€æœºåˆ¶"""
    if not text.strip():
        return None, "âŒ Text cannot be empty."

    # 1. Primary Engine: Edge TTS
    async def try_edge_tts():
        try:
            rate_str = f"{speed_int:+d}%"
            communicate = edge_tts.Communicate(text, voice_id, rate=rate_str)
            mp3_fp = io.BytesIO()
            audio_received = False
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    mp3_fp.write(chunk["data"])
                    audio_received = True
            if not audio_received:
                return None, "Edge Error: No audio was received."
            return mp3_fp.getvalue(), None
        except Exception as e:
            return None, f"Edge Error: {e}"

    # 2. Fallback/Alternative: gTTS
    def try_gtts():
        try:
            lang_map = { "ğŸ‡¬ğŸ‡§ è‹±è¯­": "en", "ğŸ‡·ğŸ‡º ä¿„è¯­": "ru", "ğŸ‡«ğŸ‡· æ³•è¯­": "fr", "ğŸ‡©ğŸ‡ª å¾·è¯­": "de" }
            lang_code = lang_map.get(cfg.get("learn_lang", "ğŸ‡¬ğŸ‡§ è‹±è¯­"), "en")

            mp3_fp = io.BytesIO()
            tts = gTTS(text, lang=lang_code)
            tts.write_to_fp(mp3_fp)
            return mp3_fp.getvalue(), None
        except Exception as e:
            return None, f"gTTS Error: {e}"

    # 3. Alternative: OpenAI TTS
    def try_openai_tts():
        try:
            client, err = get_api_client(cfg)
            if not client: return None, err

            openai_voice = "nova" # Default female voice
            if "Ryan" in voice_id or "Dmitry" in voice_id or "Henri" in voice_id or "Conrad" in voice_id:
                openai_voice = "echo" # Switch to a male voice

            speed_float = max(0.25, min(4.0, 1.0 + (speed_int / 100.0)))

            response = client.audio.speech.create(model="tts-1", voice=openai_voice, speed=speed_float, input=text)
            return response.content, None
        except Exception as e:
            return None, f"OpenAI TTS Error: {e}"

    # Main Logic
    if "Edge TTS" in engine_type:
        audio, err = await try_edge_tts()
        if audio:
            return audio, None
        st.warning("âš ï¸ Edge TTS failed, falling back to gTTS...")
        return try_gtts()

    elif "OpenAI TTS" in engine_type:
        return try_openai_tts()

    elif "gTTS" in engine_type:
        return try_gtts()

    return None, "Unsupported Engine"

async def create_anki_package(selected_items, cfg):
    """Anki å¯¼å‡ºæ ¸å¿ƒ"""
    deck = genanki.Deck(random.randrange(1<<30, 1<<31), 'è·Ÿè¯»åŠ©æ‰‹ç”Ÿè¯æœ¬')
    model = genanki.Model(1607392319, 'Simple Model', fields=[{'name': 'Q'}, {'name': 'A'}, {'name': 'Media'}],
        templates=[{'name': 'Card', 'qfmt': '{{Q}}<br>{{Media}}', 'afmt': '{{FrontSide}}<hr>{{A}}'}])
    media, temp_paths = [], []
    
    for item in selected_items:
        v_role = "ru-RU-DmitryNeural" if "ru" in str(item) else "en-US-AriaNeural"
        aud, _ = await get_audio_bytes_mixed(item['word'], "Edge (æ¨è)", v_role, 0, cfg)
        fname = ""
        if aud:
            fname = f"anki_{random.randint(10000,99999)}.mp3"
            with open(fname, "wb") as f: f.write(aud)
            media.append(fname); temp_paths.append(fname)
        
        deck.add_note(genanki.Note(model=model, fields=[
            f"{item['word']} <span style='color:grey'>[{item.get('ipa','')}]</span>",
            f"ğŸ‡¨ğŸ‡³ {item.get('zh','')}<br>ğŸ‡·ğŸ‡º {item.get('ru','')}",
            f"[sound:{fname}]" if fname else ""
        ]))
    
    out = io.BytesIO()
    genanki.Package(deck, media_files=media).write_to_file(out)
    for p in temp_paths: os.remove(p)
    out.seek(0)
    return out

# ================= 5. ä¾§è¾¹æ  =================

with st.sidebar:
    st.title("Reading Pro")
    page = option_menu(None, ["å­¦ä¹ ä¸»é¡µ", "å•è¯æœ¬", "è®¾ç½®"], icons=['book', 'bookmark', 'gear'])
    
    st.divider()
    
    # è‡ªåŠ¨è¯­è¨€åŒæ­¥é€»è¾‘
    lang_map = {
        "ğŸ‡¬ğŸ‡§ è‹±è¯­": {"default": "en-US-AriaNeural", "voices": {"ğŸ‡ºğŸ‡¸ Aria": "en-US-AriaNeural", "ğŸ‡¬ğŸ‡§ Ryan": "en-GB-RyanNeural"}},
        "ğŸ‡·ğŸ‡º ä¿„è¯­": {"default": "ru-RU-DmitryNeural", "voices": {"ğŸ‡·ğŸ‡º Dmitry": "ru-RU-DmitryNeural", "ğŸ‡·ğŸ‡º Svetlana": "ru-RU-SvetlanaNeural"}},
        "ğŸ‡«ğŸ‡· æ³•è¯­": {"default": "fr-FR-HenriNeural", "voices": {"ğŸ‡«ğŸ‡· Henri": "fr-FR-HenriNeural", "ğŸ‡«ğŸ‡· Denise": "fr-FR-DeniseNeural"}},
        "ğŸ‡©ğŸ‡ª å¾·è¯­": {"default": "de-DE-ConradNeural", "voices": {"ğŸ‡©ğŸ‡ª Conrad": "de-DE-ConradNeural"}}
    }
    
    # æ¸²æŸ“è¯­è¨€é€‰æ‹©å™¨
    lang_list = list(lang_map.keys())
    curr_lang = st.session_state.cfg.get("learn_lang", "ğŸ‡¬ğŸ‡§ è‹±è¯­")
    
    # å¦‚æœæ£€æµ‹åˆ° text å˜åŒ–ï¼Œè¿™é‡Œå°è¯•è‡ªåŠ¨è·³è½¬
    if st.session_state.main_text:
        detected = auto_detect_language(st.session_state.main_text)
        # åªæœ‰å½“æ£€æµ‹å‡ºçš„è¯­è¨€åœ¨åˆ—è¡¨é‡Œï¼Œä¸”å’Œå½“å‰ä¸åŒï¼Œæ‰è‡ªåŠ¨åˆ‡æ¢
        if detected in lang_list and detected != curr_lang:
            curr_lang = detected
            st.session_state.cfg["learn_lang"] = detected
            st.session_state.cfg["voice_role"] = lang_map[detected]["default"]
            st.toast(f"ğŸ” å·²è‡ªåŠ¨åˆ‡æ¢åˆ°: {detected}")

    sel_lang = st.selectbox("ç›®æ ‡è¯­è¨€", lang_list, index=lang_list.index(curr_lang) if curr_lang in lang_list else 0)
    
    # å¦‚æœç”¨æˆ·æ‰‹åŠ¨æ”¹äº† Dropdown
    if sel_lang != st.session_state.cfg["learn_lang"]:
        st.session_state.cfg["learn_lang"] = sel_lang
        st.session_state.cfg["voice_role"] = lang_map[sel_lang]["default"]
        st.rerun()
        
    # æ¸²æŸ“éŸ³è‰²é€‰æ‹©
    curr_voices_dict = lang_map[st.session_state.cfg["learn_lang"]]["voices"]
    v_names = list(curr_voices_dict.keys())
    
    # åæŸ¥å½“å‰ voice name
    curr_v_code = st.session_state.cfg.get("voice_role")
    curr_v_name = v_names[0]
    for name, code in curr_voices_dict.items():
        if code == curr_v_code: curr_v_name = name
        
    sel_v_name = st.selectbox("å‘éŸ³äºº", v_names, index=v_names.index(curr_v_name) if curr_v_name in v_names else 0)
    st.session_state.cfg["voice_role"] = curr_voices_dict[sel_v_name]
    
    st.session_state.cfg["speed"] = st.slider("è¯­é€Ÿ", -50, 50, st.session_state.cfg["speed"], 10)

    engine_options = ["Edge TTS", "OpenAI TTS", "gTTS"]
    current_engine = st.session_state.cfg.get("engine", "Edge TTS")
    try:
        default_index = engine_options.index(current_engine)
    except ValueError:
        default_index = 0 # Default to "Edge TTS" if the old value is not found

    st.session_state.cfg["engine"] = st.selectbox("è¯­éŸ³å¼•æ“",
        engine_options,
        index=default_index
    )

# ================= 6. ä¸»é¡µé¢é€»è¾‘ =================

if page == "å­¦ä¹ ä¸»é¡µ":
    c1, c2 = st.columns([2, 1])
    
    with c1:
        st.subheader("ğŸ“ åŸæ–‡")
        # ä¿®å¤ï¼šç›´æ¥ç»‘å®š session_stateï¼Œæ— éœ€ Update æŒ‰é’®
        st.session_state.main_text = st.text_area("Input", value=st.session_state.main_text, height=250, label_visibility="collapsed", placeholder="åœ¨æ­¤ç²˜è´´æ–‡ç« ...")
        
        # æ“ä½œåŒº
        if st.session_state.main_text:
            col_ops = st.columns([1, 1, 2])
            with col_ops[0]:
                if st.button("â–¶ï¸ æœ—è¯»", type="primary", use_container_width=True):
                    with st.spinner("ç”Ÿæˆè¯­éŸ³..."):
                        ab, err = asyncio.run(get_audio_bytes_mixed(
                            st.session_state.main_text,
                            st.session_state.cfg["engine"],
                            st.session_state.cfg["voice_role"],
                            st.session_state.cfg["speed"],
                            st.session_state.cfg
                        ))
                        if ab: 
                            st.session_state.audio_data = ab
                            st.session_state.audio_timestamp = time.time() # å¼ºåˆ¶åˆ·æ–°
                            st.rerun()
                        else: st.error(err)
            
            with col_ops[1]:
                if st.button("ğŸŒ ç¿»è¯‘", use_container_width=True):
                    with st.spinner("ç¿»è¯‘ä¸­..."):
                        res, err = api_call("trans", st.session_state.main_text, st.session_state.cfg)
                        if res: st.session_state.trans_text = res; st.rerun()
                        else: st.error(err)
            
            # æ’­æ”¾å™¨æ¸²æŸ“ (å…³é”®ä¿®å¤: ä½¿ç”¨ key)
            if st.session_state.audio_data:
                st.audio(st.session_state.audio_data, format="audio/mp3", autoplay=True)
                # ä¸‹è½½é“¾æ¥
                b64 = base64.b64encode(st.session_state.audio_data).decode()
                fname = get_smart_filename(st.session_state.main_text)
                st.markdown(f'<a href="data:audio/mp3;base64,{b64}" download="{fname}" style="text-decoration:none;">ğŸ“¥ ä¸‹è½½éŸ³é¢‘</a>', unsafe_allow_html=True)

            if st.session_state.trans_text:
                st.info(st.session_state.trans_text)

    with c2:
        st.subheader("ğŸ” æŸ¥è¯")
        with st.container(border=True):
            q = st.text_input("Word", placeholder="è¾“å…¥å•è¯...")
            if st.button("æŸ¥è¯¢", use_container_width=True):
                if q:
                    with st.spinner("Looking up..."):
                        # 1. æŸ¥ä¹‰
                        info, err = api_call("lookup", q, st.session_state.cfg)
                        if info:
                            st.session_state.last_lookup = info
                            # 2. è‡ªåŠ¨ç”Ÿæˆå‘éŸ³
                            # æŸ¥è¯å‘éŸ³å§‹ç»ˆç”¨è‹±è¯­æˆ–æ ¹æ®å•è¯æ£€æµ‹
                            v_role = "en-US-AriaNeural"
                            if re.search(r'[\u0400-\u04FF]', q): v_role = "ru-RU-DmitryNeural"
                            
                            ab, _ = asyncio.run(get_audio_bytes_mixed(q, st.session_state.cfg["engine"], v_role, 0, st.session_state.cfg))
                            st.session_state.lookup_audio = ab
                            st.session_state.lookup_audio_ts = time.time() # å¼ºåˆ¶åˆ·æ–°
                            
                            # 3. å­˜å…¥ç”Ÿè¯æœ¬
                            if not any(x['word'] == q for x in st.session_state.vocab):
                                st.session_state.vocab.insert(0, {**info, "date": datetime.now().strftime("%Y-%m-%d")})
                                save_vocab(st.session_state.vocab)
                            st.rerun()
                        else: st.error(err)

        if st.session_state.last_lookup:
            info = st.session_state.last_lookup

            # Custom container to simulate the card with the button inside
            st.markdown('<div class="lookup-card">', unsafe_allow_html=True)

            col1, col2 = st.columns([4, 1])
            with col1:
                st.markdown(f"""
                    <h3 style="margin:0;">{info.get('word', '')}</h3>
                    <div style="color:var(--secondary-text-color); font-family:monospace;">[{info.get('ipa', '')}]</div>
                """, unsafe_allow_html=True)

            with col2:
                # This button is now inline with the word
                if st.button("ğŸ”Š", key=f"play_lookup_{info.get('word')}"):
                    v_role = "en-US-AriaNeural"
                    if re.search(r'[\u0400-\u04FF]', info['word']):
                        v_role = "ru-RU-DmitryNeural"
                    ab, err = asyncio.run(get_audio_bytes_mixed(info['word'], st.session_state.cfg["engine"], v_role, 0, st.session_state.cfg))
                    if ab:
                        st.session_state.lookup_audio = ab
                        st.session_state.lookup_audio_ts = time.time()
                        st.rerun()
                    else:
                        st.error(err)

            st.markdown(f"""
                <hr style="margin:10px 0; border:none; border-top:1px solid var(--card-border-color);">
                <div><b>ğŸ‡¨ğŸ‡³</b> {info.get('zh','')}</div>
                <div style="margin-top:5px"><b>ğŸ‡·ğŸ‡º</b> {info.get('ru','')}</div>
            """, unsafe_allow_html=True)

            st.markdown('</div>', unsafe_allow_html=True)
            
            # æŸ¥è¯ä¸“ç”¨æ’­æ”¾å™¨ (ä¸å¯è§ï¼Œä»…è‡ªåŠ¨æ’­æ”¾)
            if st.session_state.lookup_audio:
                st.audio(st.session_state.lookup_audio, format="audio/mp3", autoplay=True)

elif page == "å•è¯æœ¬":
    st.subheader(f"ğŸ““ ç”Ÿè¯æœ¬ ({len(st.session_state.vocab)})")
    if st.button("ğŸ“¤ å¯¼å‡º Anki åŒ…"):
        sel = [v for i,v in enumerate(st.session_state.vocab) if st.session_state.get(f"chk_{i}", False)]
        if not sel: st.warning("è¯·å…ˆå‹¾é€‰")
        else:
            dat = asyncio.run(create_anki_package(sel, st.session_state.cfg))
            st.download_button("â¬‡ï¸ ä¸‹è½½ .apkg", dat, file_name="vocab.apkg")

    st.markdown("---")
    for i, item in enumerate(st.session_state.vocab):
        c1, c2, c3, c4, c5 = st.columns([0.5, 2, 3, 1, 1])
        c1.checkbox("", key=f"chk_{i}")
        c2.markdown(f"**{item['word']}**")
        c3.text(f"{item.get('zh','')} {item.get('ru','')}")
        if c4.button("ğŸ”Š", key=f"v_play_{i}"):
            v_role = "en-US-AriaNeural"
            if re.search(r'[\u0400-\u04FF]', item['word']): v_role = "ru-RU-DmitryNeural"
            ab, _ = asyncio.run(get_audio_bytes_mixed(item['word'], st.session_state.cfg["engine"], v_role, 0, st.session_state.cfg))
            st.session_state.lookup_audio = ab
            st.session_state.lookup_audio_ts = time.time()
            st.rerun()
        if c5.button("ğŸ—‘ï¸", key=f"v_del_{i}"):
            st.session_state.vocab.pop(i)
            save_vocab(st.session_state.vocab)
            st.rerun()
            
    # å¤ç”¨æŸ¥è¯æ’­æ”¾å™¨
    if st.session_state.lookup_audio:
        st.audio(st.session_state.lookup_audio, format="audio/mp3", autoplay=True)

elif page == "è®¾ç½®":
    st.subheader("âš™ï¸ æ¨¡å‹ä¸æ¥å£é…ç½®")
    st.text_input("API Key", value=st.session_state.cfg["api_key"], type="password", key="key_input", on_change=lambda: st.session_state.cfg.update({"api_key": st.session_state.key_input}))
    st.text_input("Base URL", value=st.session_state.cfg["generic_base_url"], key="url_input", on_change=lambda: st.session_state.cfg.update({"generic_base_url": st.session_state.url_input}))

    st.divider()

    # LLM Model Selection
    chat_models = ["deepseek-ai/DeepSeek-V3"]
    selected_chat_model = st.selectbox(
        "LLM (Chat) Model",
        chat_models,
        index=chat_models.index(st.session_state.cfg.get("chat_model", chat_models[0]))
    )
    st.session_state.cfg["chat_model"] = selected_chat_model

    # OCR Model Selection
    ocr_models = ["Qwen/Qwen2-VL-72B-Instruct"]
    selected_ocr_model = st.selectbox(
        "OCR (Vision) Model",
        ocr_models,
        index=ocr_models.index(st.session_state.cfg.get("ocr_model", ocr_models[0]))
    )
    st.session_state.cfg["ocr_model"] = selected_ocr_model