import streamlit as st
from openai import OpenAI
import edge_tts
import asyncio
import json
import os
import io
import base64
import random
from datetime import datetime
from PIL import Image
import genanki
import socket

# ================= 1. ç¯å¢ƒä¸é…ç½® =================

for key in ["all_proxy", "http_proxy", "https_proxy"]:
    if key in os.environ: del os.environ[key]
os.environ["no_proxy"] = "localhost,127.0.0.1,::1"

st.set_page_config(page_title="è·Ÿè¯»åŠ©æ‰‹ Pro (V11.0 ä¿®å¤ç‰ˆ)", layout="wide", page_icon="ğŸ¦‹")

VOCAB_FILE = "my_vocab.json"
# ç§»é™¤æœ¬åœ° config è¯»å†™ï¼Œç¡®ä¿äº‘ç«¯å®‰å…¨
# CONFIG_FILE = "config.json"

def get_local_ip():
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except: return "127.0.0.1"

def load_config():
    config = {
        "chat_model": "deepseek-ai/DeepSeek-V3",
        "ocr_model": "Qwen/Qwen2.5-VL-72B-Instruct",
        "trans_prompt": "Translate the following text into fluent, natural Chinese.",
        "api_key": "",
        "sf_tts_model_id": "FunAudioLLM/CosyVoice2-0.5B" 
    }
    # ä»…ä» Secrets è¯»å–
    try:
        if "SILICON_KEY" in st.secrets: config["api_key"] = st.secrets["SILICON_KEY"]
    except: pass
    return config

if 'app_config' not in st.session_state:
    st.session_state.app_config = load_config()

# ================= 2. æ ¸å¿ƒæ•°æ® =================

VOICE_MAP_EDGE = {
    "ğŸ‡¬ğŸ‡§ è‹±è¯­": [("en-GB-RyanNeural", "Ryan (è‹±/ç”·)"), ("en-US-ChristopherNeural", "Chris (ç¾/ç”·)"), ("en-US-AriaNeural", "Aria (ç¾/å¥³)")],
    "ğŸ‡«ğŸ‡· æ³•è¯­": [("fr-FR-HenriNeural", "Henri (æ³•/ç”·)"), ("fr-FR-DeniseNeural", "Denise (æ³•/å¥³)")],
    "ğŸ‡©ğŸ‡ª å¾·è¯­": [("de-DE-ConradNeural", "Conrad (å¾·/ç”·)"), ("de-DE-KatjaNeural", "Katja (å¾·/å¥³)")],
    "ğŸ‡·ğŸ‡º ä¿„è¯­": [("ru-RU-DmitryNeural", "Dmitry (ä¿„/ç”·)"), ("ru-RU-SvetlanaNeural", "Svetlana (ä¿„/å¥³)")],
}

VOICE_MAP_SF = {
    "ç”·å£° - Benjamin (è‹±ä¼¦é£)": "FunAudioLLM/CosyVoice2-0.5B:benjamin", 
    "ç”·å£° - Alex (æ²‰ç¨³)": "FunAudioLLM/CosyVoice2-0.5B:alex",
    "ç”·å£° - Bob (æ¬¢å¿«)": "FunAudioLLM/CosyVoice2-0.5B:bob",
    "å¥³å£° - Anna (æ–°é—»)": "FunAudioLLM/CosyVoice2-0.5B:anna",
    "å¥³å£° - Bella (æ¸©æŸ”)": "FunAudioLLM/CosyVoice2-0.5B:bella",
    "å¥³å£° - Claire (æ¸…æ™°)": "FunAudioLLM/CosyVoice2-0.5B:claire"
}

def load_vocab():
    if os.path.exists(VOCAB_FILE):
        try: return json.load(open(VOCAB_FILE, "r", encoding="utf-8"))
        except: return []
    return []

def save_vocab(vocab_list):
    try: json.dump(vocab_list, open(VOCAB_FILE, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
    except: pass

def compress_image(image):
    image.thumbnail((1024, 1024)); buffered = io.BytesIO(); image.save(buffered, format="JPEG", quality=85)
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

# ================= 3. éŸ³é¢‘å¤„ç†æ ¸å¿ƒ (ä¿®å¤è¯­é€Ÿ) =================

async def get_audio_bytes_mixed(text, engine_type, voice_id, speed_int, app_config):
    """
    speed_int: -50 åˆ° 50 çš„æ•´æ•°
    """
    
    # 1. Edge TTS (ä½¿ç”¨ç™¾åˆ†æ¯”è¯­é€Ÿ)
    if "Edge" in engine_type:
        rate_str = f"{speed_int:+d}%" # ä¾‹å¦‚ "+10%"
        try:
            communicate = edge_tts.Communicate(text, voice_id, rate=rate_str)
            mp3_fp = io.BytesIO()
            async for chunk in communicate.stream():
                if chunk["type"] == "audio": mp3_fp.write(chunk["data"])
            return mp3_fp.getvalue(), None
        except Exception as e: return None, f"Edge ({voice_id}) å¤±è´¥: {e}"

    # 2. SiliconFlow (ä½¿ç”¨æµ®ç‚¹æ•°è¯­é€Ÿ)
    elif "SiliconFlow" in engine_type:
        api_key = app_config["api_key"]
        if not api_key: return None, "è¯·å…ˆè¾“å…¥ API Key"
        client = OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")
        
        # è§£ææ¨¡å‹
        model_id = "FunAudioLLM/CosyVoice2-0.5B"
        if ":" in voice_id: model_id = voice_id.split(":")[0]

        # ğŸ”¥ ä¿®å¤è¯­é€Ÿ: å°† -50~50 æ˜ å°„ä¸º 0.5~1.5
        # 0 -> 1.0 (åŸé€Ÿ)
        # 50 -> 1.5 (1.5å€é€Ÿ)
        # -50 -> 0.5 (0.5å€é€Ÿ)
        speed_float = 1.0 + (speed_int / 100.0)

        try:
            response = client.audio.speech.create(
                model=model_id,
                voice=voice_id,
                input=text,
                speed=speed_float # ä¼ å…¥è®¡ç®—åçš„æµ®ç‚¹æ•°
            )
            return response.content, None
        except Exception as e: 
            return None, f"SF TTS å¤±è´¥: {e}"

    return None, "æœªçŸ¥å¼•æ“"

# ================= 4. Anki å¯¼å‡º (ä¿®å¤å†…å®¹ç¼ºå¤± & å¼•æ“åŒæ­¥) =================

async def create_anki_package(selected_items, engine_type, voice_id, speed_int, app_config):
    """
    å®Œå…¨ä¿®å¤çš„ Anki æ‰“åŒ…å‡½æ•°
    1. ä¼ å…¥å½“å‰å¼•æ“è®¾ç½®ï¼Œç¡®ä¿ç”Ÿæˆçš„éŸ³é¢‘å’Œå¬çš„ä¸€æ ·ã€‚
    2. ä¿®å¤å­—æ®µæ˜ å°„ï¼ŒåŒ…å« IPA å’Œ ä¿„è¯­ã€‚
    """
    deck_id = random.randrange(1 << 30, 1 << 31)
    deck = genanki.Deck(deck_id, 'è·Ÿè¯»åŠ©æ‰‹ç”Ÿè¯æœ¬')
    
    # ä¿®å¤ Model å­—æ®µï¼šå¢åŠ  IPA å’Œ RU
    my_model = genanki.Model(
        random.randrange(1 << 30, 1 << 31),
        'Simple Model with Audio',
        fields=[
            {'name': 'Question'}, 
            {'name': 'Answer'}, 
            {'name': 'Audio'}
        ],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '{{Question}}<br>{{Audio}}', # æ­£é¢ï¼šå•è¯+éŸ³æ ‡+å‘éŸ³
                'afmt': '{{FrontSide}}<hr id="answer">{{Answer}}', # èƒŒé¢ï¼šé‡Šä¹‰
            }
        ])

    media_files = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, item in enumerate(selected_items):
        status_text.text(f"æ­£åœ¨ç”ŸæˆéŸ³é¢‘: {item['word']}...")
        
        # 1. ç”ŸæˆéŸ³é¢‘ (å¤ç”¨æ ¸å¿ƒå‡½æ•°ï¼Œç¡®ä¿å¼•æ“ä¸€è‡´)
        audio_data, _ = await get_audio_bytes_mixed(
            item['word'], engine_type, voice_id, speed_int, app_config
        )
        
        audio_filename = ""
        if audio_data:
            audio_filename = f"anki_audio_{random.randint(1000,9999)}_{idx}.mp3"
            # å†™å…¥æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ç»™ genanki è¯»å–
            with open(audio_filename, "wb") as f:
                f.write(audio_data)
            media_files.append(audio_filename)
        
        # 2. å‡†å¤‡å†…å®¹ (ä¿®å¤å†…å®¹ç¼ºå¤±)
        # æ­£é¢ï¼šå•è¯ + éŸ³æ ‡ (ç°è‰²å°å­—)
        word_field = f"{item['word']} <br> <span style='color:grey; font-size: 0.8em;'>{item.get('ipa', '')}</span>"
        
        # èƒŒé¢ï¼šä¸­æ–‡ + ä¿„è¯­ (æ¢è¡Œ)
        meaning_field = f"ğŸ‡¨ğŸ‡³ {item.get('zh', '')} <br> ğŸ‡·ğŸ‡º {item.get('ru', '')}"
        
        # éŸ³é¢‘å­—æ®µ
        audio_field = f"[sound:{audio_filename}]" if audio_filename else ""

        # 3. æ·»åŠ ç¬”è®°
        note = genanki.Note(
            model=my_model,
            fields=[word_field, meaning_field, audio_field]
        )
        deck.add_note(note)
        
        progress_bar.progress((idx + 1) / len(selected_items))

    # æ‰“åŒ…
    status_text.text("æ­£åœ¨æ‰“åŒ… .apkg æ–‡ä»¶...")
    output_package = genanki.Package(deck)
    output_package.media_files = media_files
    
    # å†™å…¥å†…å­˜æµ
    pkg_bytes = io.BytesIO()
    # genanki éœ€è¦å†™ä¸´æ—¶æ–‡ä»¶
    temp_pkg_name = "temp_anki_output.apkg"
    output_package.write_to_file(temp_pkg_name)
    
    with open(temp_pkg_name, "rb") as f:
        final_bytes = f.read()
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(temp_pkg_name)
    for f in media_files:
        if os.path.exists(f): os.remove(f)
        
    progress_bar.empty()
    status_text.empty()
    return final_bytes

# ================= 5. API æŸ¥è¯ä¸ç¿»è¯‘ =================
def silicon_ocr_multilang(image, api_key, model_id):
    client = OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1"); base64_image = compress_image(image)
    try: response = client.chat.completions.create(model=model_id, messages=[{"role": "user", "content": [{"type": "text", "text": "Extract all legible text. Keep original language."}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}]}]); return response.choices[0].message.content, None
    except Exception as e: return None, str(e)
def silicon_vocab_lookup_multilang(word, api_key, model_id):
    client = OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")
    prompt = f"""Dictionary API. User input: "{word}". Return JSON: {{ "detected_lang": "...", "ipa": "...", "zh": "...", "ru": "..." }} (Concise)"""
    try: response = client.chat.completions.create(model=model_id, messages=[{"role": "user", "content": prompt}], response_format={"type": "json_object"}); return json.loads(response.choices[0].message.content), None
    except Exception as e: return None, str(e)
def silicon_translate_text(text, api_key, model_id, system_prompt):
    client = OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1"); full_prompt = f"{system_prompt}\n\n{text}"
    try: response = client.chat.completions.create(model=model_id, messages=[{"role": "user", "content": full_prompt}]); return response.choices[0].message.content, None
    except Exception as e: return None, str(e)

# ================= 6. ç•Œé¢ UI =================

st.title("ğŸ¦‹ è·Ÿè¯»åŠ©æ‰‹ (V11.0)")

if 'vocab_book' not in st.session_state: st.session_state.vocab_book = load_vocab()
if 'current_text' not in st.session_state: st.session_state.current_text = ""
if 'audio_cache' not in st.session_state: st.session_state.audio_cache = None
if 'translation_result' not in st.session_state: st.session_state.translation_result = ""
if 'temp_word_audio' not in st.session_state: st.session_state.temp_word_audio = {}

with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    
    # Key (ä¸ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶)
    default_key = st.session_state.app_config.get("api_key", "")
    api_input = st.text_input("SiliconFlow Key", value=default_key, type="password")
    if api_input != st.session_state.app_config.get("api_key"):
        st.session_state.app_config["api_key"] = api_input

    st.divider()
    tts_engine = st.radio("ğŸ”Š è¯­éŸ³å¼•æ“", ["Edge (æ¨è/å…è´¹)", "SiliconFlow (ä»˜è´¹)"], index=0)
    
    voice_id = "default"
    if tts_engine == "SiliconFlow (ä»˜è´¹)":
        st.info("ğŸ’ CosyVoice2 (æ”¯æŒå€é€Ÿ)")
        voice_choice = st.selectbox("ğŸ™ï¸ é€‰æ‹©éŸ³è‰²", list(VOICE_MAP_SF.keys()))
        voice_id = VOICE_MAP_SF[voice_choice]
        
    elif tts_engine == "Edge (æ¨è/å…è´¹)":
        lang_choice_temp = st.selectbox("ğŸŒ è¯­è¨€é¢„è§ˆ", list(VOICE_MAP_EDGE.keys()), index=0, key="edge_lang_prev")
        available_voices = VOICE_MAP_EDGE[lang_choice_temp]
        voice_id = st.radio("ğŸ™ï¸ éŸ³è‰²", [v[0] for v in available_voices], format_func=lambda x: next(v[1] for v in available_voices if v[0] == x))

    st.divider()
    lang_choice = st.selectbox("ğŸŒ å­¦ä¹ è¯­è¨€", list(VOICE_MAP_EDGE.keys()), index=0)
    speed_int = st.slider("ğŸ‡ è¯­é€Ÿè°ƒèŠ‚", -50, 50, 0, 5, help="Edge: ç™¾åˆ†æ¯” | CosyVoice: 0.5x-1.5x")
    
    if not api_input: st.warning("âš ï¸ è¯·è¾“å…¥ Key"); st.stop()

# --- ä¸»ç•Œé¢ ---
col1, col2 = st.columns([1.6, 1.4])

with col1:
    st.subheader("1. å­¦ä¹ å†…å®¹")
    tab_ocr, tab_txt = st.tabs(["ğŸ“· æ‹ç…§è¯†åˆ«", "âœï¸ æ‰‹åŠ¨è¾“å…¥"])
    with tab_ocr:
        uploaded = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=['jpg', 'png'])
        if uploaded and st.button("å¼€å§‹è¯†åˆ«"):
            img = Image.open(uploaded)
            res, _ = silicon_ocr_multilang(img, api_input, "Qwen/Qwen2.5-VL-72B-Instruct")
            if res: st.session_state.current_text = res; st.session_state.translation_result = ""; st.rerun()
    with tab_txt:
        txt = st.text_area("è¾“å…¥æ–‡æœ¬", height=100)
        if st.button("ç¡®è®¤"): st.session_state.current_text = txt; st.rerun()

    if st.session_state.current_text:
        st.markdown("---")
        final_text = st.text_area("æ­£æ–‡", value=st.session_state.current_text, height=200)
        
        if st.button(f"â–¶ï¸ æ’­æ”¾è¯­éŸ³", type="primary", use_container_width=True):
            with st.spinner(f"æ­£åœ¨ç”Ÿæˆ..."):
                # ğŸ”¥ ä¼ å…¥ speed_int
                ab, err = asyncio.run(get_audio_bytes_mixed(
                    final_text, tts_engine, voice_id, speed_int, st.session_state.app_config
                ))
                if ab: st.session_state.audio_cache = ab; st.rerun()
                else: st.error(err)
        
        if st.session_state.audio_cache:
            st.audio(st.session_state.audio_cache, format='audio/mpeg')

        with st.expander("ğŸ‡¨ğŸ‡³ å…¨æ–‡ç¿»è¯‘", expanded=False):
            if st.button("ğŸš€ ç¿»è¯‘"):
                res, _ = silicon_translate_text(final_text, api_input, "deepseek-ai/DeepSeek-V3", "Translate to Chinese.")
                if res: st.info(res)

with col2:
    st.subheader("ğŸ“š æ™ºèƒ½å•è¯æœ¬")
    with st.form("lookup"):
        c1, c2 = st.columns([3,1])
        wq = c1.text_input("æŸ¥è¯", label_visibility="collapsed")
        if c2.form_submit_button("ğŸ”"):
            info, _ = silicon_vocab_lookup_multilang(wq, api_input, "deepseek-ai/DeepSeek-V3")
            if info: 
                st.session_state.vocab_book.insert(0, {"word": wq, "lang": lang_choice, "date": datetime.now().strftime("%Y-%m-%d"), **info})
                save_vocab(st.session_state.vocab_book)
                st.rerun()

    st.divider()
    
    filtered_vocab = [v for v in st.session_state.vocab_book if v.get('lang', 'ğŸ‡¬ğŸ‡§ è‹±è¯­') == lang_choice]
    
    if filtered_vocab:
        checked_items = []
        grouped = {}
        for item in filtered_vocab:
            d = item.get('date', 'Unknown')
            if d not in grouped: grouped[d] = []
            grouped[d].append(item)
            
        for d, items in grouped.items():
            st.caption(f"ğŸ“… {d}")
            for idx, item in enumerate(items):
                c_chk, c_wd, c_ph = st.columns([0.1, 0.4, 0.5])
                with c_chk:
                    unique_key = f"chk_{item['word']}_{d}_{idx}" 
                    if st.checkbox("", key=unique_key): checked_items.append(item)
                with c_wd:
                    st.markdown(f"**{item['word']}**")
                    # æ˜¾ç¤º IPA
                    if item.get('ipa'): st.caption(f"[{item['ipa']}]")
                    if st.button("ğŸ”Š", key=f"p_{item['word']}_{d}_{idx}"):
                        # ğŸ”¥ å•è¯æ’­æ”¾ä¹Ÿä¼ å…¥ speed_int
                        ab, _ = asyncio.run(get_audio_bytes_mixed(item['word'], tts_engine, voice_id, speed_int, st.session_state.app_config))
                        if ab: st.session_state.temp_word_audio[item['word']] = ab; st.rerun()
                with c_ph:
                    st.markdown(f"ğŸ‡¨ğŸ‡³ {item.get('zh','')}")
                    # æ˜¾ç¤ºä¿„è¯­
                    st.markdown(f"ğŸ‡·ğŸ‡º {item.get('ru','')}")
                
                if item['word'] in st.session_state.temp_word_audio:
                    st.audio(st.session_state.temp_word_audio[item['word']], autoplay=True)
                    del st.session_state.temp_word_audio[item['word']]
            st.divider()

        if checked_items:
            st.info(f"é€‰ä¸­ {len(checked_items)} ä¸ªå•è¯")
            col_exp, col_del = st.columns(2)
            with col_exp:
                if st.button("ğŸ“¤ å¯¼å‡ºAnki (å¸¦éŸ³é¢‘)"):
                    with st.spinner("æ­£åœ¨ç”ŸæˆAnkiåŒ… (åŒ…å«éŸ³é¢‘)..."):
                        # ğŸ”¥ ä¼ å…¥æ‰€æœ‰é…ç½®å‚æ•°ï¼Œç¡®ä¿AnkiéŸ³é¢‘å’Œå½“å‰è®¾ç½®ä¸€è‡´
                        apkg_bytes = asyncio.run(create_anki_package(
                            checked_items, tts_engine, voice_id, speed_int, st.session_state.app_config
                        ))
                        st.download_button("â¬‡ï¸ ä¸‹è½½ .apkg", data=apkg_bytes, file_name=f"anki_{datetime.now().strftime('%m%d')}.apkg")
            with col_del:
                if st.button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­"):
                    rem_words = [i['word'] for i in checked_items]
                    st.session_state.vocab_book = [i for i in st.session_state.vocab_book if i['word'] not in rem_words]
                    save_vocab(st.session_state.vocab_book)
                    st.rerun()
    else:
        st.caption(f"æš‚æ—  {lang_choice} ç”Ÿè¯")
